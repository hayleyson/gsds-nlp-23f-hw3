{"cells":[{"cell_type":"markdown","metadata":{"id":"291xnlw4LtVi"},"source":["# **RUN this cell before you start**"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":9596,"status":"ok","timestamp":1696852338799,"user":{"displayName":"­박성일 / 학생 / 데이터사이언스학과","userId":"16444274223260770728"},"user_tz":-540},"id":"7xCFo6lILmb6"},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import nltk\n","import re\n","from nltk.corpus import stopwords\n","from collections import Counter\n","import string\n","import matplotlib.pyplot as plt\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","import urllib"]},{"cell_type":"markdown","metadata":{"id":"u3qjcxIKLsXx"},"source":["# Topic Classfication with RNN (35 pts)\n","We will implement RNN for topic classification task."]},{"cell_type":"markdown","metadata":{"id":"UpdU_tt1MM1G"},"source":["## Preprocessing\n","**Do not modify the preprocessing codes.**\n","You only need to run the cells below."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":449,"status":"ok","timestamp":1696852339245,"user":{"displayName":"­박성일 / 학생 / 데이터사이언스학과","userId":"16444274223260770728"},"user_tz":-540},"id":"3EvbZhKJLszi"},"outputs":[],"source":["# Download dataset\n","TRAIN_URL = \"https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\"\n","urllib.request.urlretrieve(TRAIN_URL, \"train.csv\")\n","i2w = {0:\"World\", 1:\"Sports\", 2:\"Business\", 3:\"Sci/Tech\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dcMJUGsNMGPi"},"outputs":[],"source":["df = pd.read_csv(\"train.csv\", header=None, names=[\"label\", \"title\", \"text\"])\n","df['label'] = df['label']-1\n","X,y = df['text'].values,df['label'].values\n","x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2, stratify=y)\n","print(f'shape of train data is {x_train.shape}')\n","print(f'shape of test data is {x_test.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xoWRDu9_MLYk"},"outputs":[],"source":["nltk.download('stopwords')\n","def preprocess_string(s):\n","    # Remove all non-word characters (everything except numbers and letters)\n","    s = re.sub(r\"[^\\w\\s]\", '', s)\n","    # Replace all runs of whitespaces with no space\n","    s = re.sub(r\"\\s+\", '', s)\n","    # replace digits with no space\n","    s = re.sub(r\"\\d\", '', s)\n","    s = re.sub('br','', s)\n","\n","    return s\n","\n","def tokenize(x_train,y_train,x_val,y_val):\n","    word_list = []\n","\n","    stop_words = set(stopwords.words('english'))\n","    for sent in x_train:\n","        for word in sent.lower().split():\n","            word = preprocess_string(word)\n","            if word not in stop_words and word != '':\n","                word_list.append(word)\n","\n","    corpus = Counter(word_list)\n","    # sorting on the basis of most common words\n","    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:50000]\n","    # creating a dict\n","    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n","\n","    # tokenize\n","    final_list_train,final_list_test = [],[]\n","    for sent in x_train:\n","            final_list_train.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split()\n","                                     if preprocess_string(word) in onehot_dict.keys()])\n","    for sent in x_val:\n","            final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split()\n","                                    if preprocess_string(word) in onehot_dict.keys()])\n","\n","    return np.array(final_list_train, dtype=object), np.array(final_list_test, dtype=object), onehot_dict\n","\n","print(\"before tokenization:\", x_train[1])\n","x_train, x_test ,vocab = tokenize(x_train,y_train,x_test,y_test)\n","print(\"after tokenization:\", x_train[1])\n","\n","print(f'Length of vocabulary is {len(vocab)}')\n","\n","def padding_(sentences, seq_len):\n","    features = np.zeros((len(sentences), seq_len),dtype=int)\n","    for ii, review in enumerate(sentences):\n","        if len(review) != 0:\n","            features[ii, -len(review):] = np.array(review)[:seq_len]\n","\n","    return features\n","\n","#we have very less number of reviews with length > 50.\n","#So we will consideronly those below it.\n","print(\"Before Padding:\", x_train[0], len(x_train[0]))\n","x_train_pad = padding_(x_train,50)\n","print(\"After Padding:\", x_train_pad[0], len(x_train_pad[0]))\n","x_test_pad = padding_(x_test,50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mdVGkE_AMrgA"},"outputs":[],"source":["train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.nn.functional.one_hot(torch.from_numpy(y_train)))\n","valid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.nn.functional.one_hot(torch.from_numpy(y_test)))\n","\n","# dataloaders\n","batch_size = 128\n","\n","# make sure to SHUFFLE your data\n","train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n","valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n","\n","dataiter = iter(train_loader)\n","sample_x, sample_y = next(iter(train_loader))\n","\n","print(\"Total number of training data :\", len(train_loader.dataset))\n","print(\"Total number of validation data :\", len(valid_loader.dataset))\n","print('Sample batch size: ', sample_x.size()) # batch_size, seq_length\n","print('Sample x: \\n', sample_x[0])\n","print('Sample y: \\n', sample_y[0])"]},{"cell_type":"markdown","metadata":{"id":"NBzI4fnxOY0D"},"source":["## Implementing an RNN model with PyTorch (15 pts)\n","In this question, we are going to implement a SimpleRNN class to classify AG news dataset. Complete the code following the instruction in the jupyter notebook file.\n","\n","\n","**(a)** Implement the `__init__()` function of **SimpleRNN** class. **(5 pts)**\n","\n","\n","**(b)** Implement the `forward()` function of SimpleRNN** class. **(10 pts)**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":875,"status":"aborted","timestamp":1696850824543,"user":{"displayName":"­박성일 / 학생 / 데이터사이언스학과","userId":"16444274223260770728"},"user_tz":-540},"id":"dsx9VsLDMu-D"},"outputs":[],"source":["# Define RNN Model\n","class SimpleRNN(nn.Module):\n","    def __init__(self, num_layers, output_dim, vocab_size, hidden_dim, embedding_dim, drop_prob=0.5):\n","        super(SimpleRNN,self).__init__()\n","        self.output_dim = output_dim\n","        self.hidden_dim = hidden_dim\n","\n","        self.num_layers = num_layers\n","        self.vocab_size = vocab_size\n","\n","        ### YOUR CODE HERE (~5 lines)\n","        ### TODO:\n","        ###     1. Initialize the RNN model using nn.RNN().\n","        ###     2. Initialize the embedding, dropout, linear, and softmax layers.\n","        ### END YOUR CODE\n","\n","    def forward(self, x):\n","        \"\"\" When sentences come in batch form, return the probability for each class.\n","        @param x (Tensor) : Tensor of padded sentences with shape (batch_size, sequence_length)\n","        @return output (Tensor): Tensor of the probability for each class with shape (batch_size, number_of_classes)\n","        \"\"\"\n","\n","        ### YOUR CODE HERE (~10 lines)\n","        ### TODO: Complete the forward pass using the layers declared in __init__ function.\n","        ### END YOUR CODE\n","\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"aborted","timestamp":1696850824851,"user":{"displayName":"­박성일 / 학생 / 데이터사이언스학과","userId":"16444274223260770728"},"user_tz":-540},"id":"0V5nF6nmPWFF"},"outputs":[],"source":["# DO NOT modify below hyperparameters. Just run the cell.\n","NUM_LAYERS = 2\n","VOCAB_SIZE = len(vocab) + 1 #extra 1 for padding\n","EMBEDDING_DIM = 128\n","OUTPUT_DIM = 4\n","HIDDEN_DIM = 256\n","LR=0.001\n","CLIP = 3\n","EPOCHS = 20\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","model = SimpleRNN(num_layers=NUM_LAYERS,\n","                  output_dim=OUTPUT_DIM,\n","                  vocab_size=VOCAB_SIZE,\n","                  hidden_dim=HIDDEN_DIM,\n","                  embedding_dim=EMBEDDING_DIM,\n","                  drop_prob=0.5)\n","\n","#moving to gpu\n","model.to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n","print(\"Device :\", device)\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"xng1DbRfQLkJ"},"source":["## Train and evaluate the RNN model (20 pts)\n","Next, we will train and evaluate the RNN model. Implement a train and evaluation code following the instruction in jupyter notebook file. \n","\n","\n","**(a)** Implement the `train()` function of Trainer class. **(8 pts)** \n","\n","\n","**(b)** After the training is complete, use the `plot()` function of Trainer class to display the figure, and then paste it here. **(4 pts)** \n","\n","\n","**(c)** Report the best validation accuracy using the print best acc() function of Trainer class. **(2 pts)** \n","\n","\n","**(d)** Based on (b) and (c), evaluate whether the training was successful, and write at least two ways to improve the model’s performance. **(6 pts)**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"aborted","timestamp":1696850824851,"user":{"displayName":"­박성일 / 학생 / 데이터사이언스학과","userId":"16444274223260770728"},"user_tz":-540},"id":"wt5yOkucQKVu"},"outputs":[],"source":["class Trainer():\n","  def __init__(self, model, train_loader, val_loader, criterion, optimizer):\n","    self.model = model\n","    self.train_loader = train_loader\n","    self.val_loader = val_loader\n","    self.criterion = criterion\n","    self.optimizer = optimizer\n","    self.epoch_train_loss, self.epoch_val_loss, self.epoch_train_acc, self.epoch_val_acc = [], [], [], []\n","\n","  def acc(self, pred, label):\n","    pred = torch.argmax(pred, dim=-1)\n","    label = torch.argmax(label, dim=-1)\n","    return torch.sum(pred == label).item()\n","\n","  def train(self, epochs: int):\n","    if self.epoch_train_loss:\n","      self.epoch_train_loss, self.epoch_val_loss, self.epoch_train_acc, self.epoch_val_acc = [], [], [], []\n","\n","    for epoch in range(epochs):\n","      ########### TRAIN ###########\n","      self.model.train()\n","      train_losses = []\n","      train_acc = 0.0\n","      for x, y in self.train_loader:\n","        ### YOUR CODE HERE (~9 lines)\n","        ### TODO:\n","        ###     1. Perform a forward pass on the SimpleRNN model.\n","        ###     2. Calculate the train loss(train_loss) and perform backpropagation.\n","        ###     3. Calculate the train accuracy(acc) using self.acc function.\n","        ###     4. Implement gradient clipping using the torch.nn.utils.clip_grad_norm_ function. You must use the 'CLIP' variable as the max_norm.\n","        ###     5. Update the model's parameters using the optimizer.\n","\n","        ### END YOUR CODE\n","        train_losses.append(train_loss)\n","        train_acc += acc\n","\n","\n","      ######### VALIDATION #########\n","      val_losses = []\n","      val_acc = 0.0\n","      self.model.eval()\n","      for x, y in self.val_loader:\n","        x, y = x.to(device), y.to(device)\n","        ### YOUR CODE HERE (~5 lines)\n","        ### TODO:\n","        ###     1. Perform a forward pass on the SimpleRNN model.\n","        ###     2. Calculate the validation loss(val_loss) and validation accuracy(acc)\n","\n","        ### END YOUR CODE\n","        val_losses.append(val_loss)\n","        val_acc += acc\n","\n","      ### DO NOT modify below codes\n","      train_loss = np.mean(train_losses)\n","      val_loss = np.mean(val_losses)\n","      train_acc = train_acc/len(self.train_loader.dataset)\n","      val_acc = val_acc/len(self.val_loader.dataset)\n","      self.epoch_train_loss.append(train_loss)\n","      self.epoch_val_loss.append(val_loss)\n","      self.epoch_train_acc.append(train_acc)\n","      self.epoch_val_acc.append(val_acc)\n","      print(f'Epoch {epoch+1}')\n","      print(f'train_loss : {train_loss} val_loss : {val_loss}')\n","      print(f'train_accuracy : {train_acc*100} val_accuracy : {val_acc*100}')\n","\n","  def print_best_acc(self):\n","    print(f\"Best accuracy -> {max(self.epoch_val_acc)*100}\")\n","\n","  def plot(self):\n","    fig = plt.figure(figsize = (20, 6))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(self.epoch_train_acc, label='Train Acc')\n","    plt.plot(self.epoch_val_acc, label='Validation Acc')\n","    plt.title(\"Accuracy\")\n","    plt.legend()\n","    plt.grid()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(self.epoch_train_loss, label='Train loss')\n","    plt.plot(self.epoch_val_loss, label='Validation loss')\n","    plt.title(\"Loss\")\n","    plt.legend()\n","    plt.grid()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2OEVGF_FMMG4"},"outputs":[],"source":["trainer= Trainer(model=model,\n","                 train_loader=train_loader,\n","                 val_loader=valid_loader,\n","                 criterion=criterion,\n","                 optimizer=optimizer)\n","trainer.train(EPOCHS)\n","trainer.print_best_acc()\n","trainer.plot()"]}],"metadata":{"colab":{"collapsed_sections":["UpdU_tt1MM1G"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
